{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnxruntime version:  1.20.1\n",
      "available providers:  ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "print(\"onnxruntime version: \", ort.__version__)\n",
    "#  available providers: 'CPUExecutionProvider', 'CUDAExecutionProvider', 'DnnlExecutionProvider', 'NupharExecutionProvider', 'TensorrtExecutionProvider', 'OpenVINOExecutionProvider', 'NnapiExecutionProvider', 'RknpuExecutionProvider', 'CoreMLExecutionProvider', 'ACLExecutionProvider', 'ArmNNExecutionProvider', 'DmlExecutionProvider', 'NnapiExecutionProvider', 'VitisAIExecutionProvider', 'HddlExecutionProvider', 'HabanaExecution\n",
    "print(\"available providers: \", ort.get_available_providers())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# print(\"onnxruntime version: \", ort.__version__)\n",
    "#  available providers: 'CPUExecutionProvider', 'CUDAExecutionProvider', 'DnnlExecutionProvider', 'NupharExecutionProvider', 'TensorrtExecutionProvider', 'OpenVINOExecutionProvider', 'NnapiExecutionProvider', 'RknpuExecutionProvider', 'CoreMLExecutionProvider', 'ACLExecutionProvider', 'ArmNNExecutionProvider', 'DmlExecutionProvider', 'NnapiExecutionProvider', 'VitisAIExecutionProvider', 'HddlExecutionProvider', 'HabanaExecution\n",
    "print( ort.get_available_providers())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/invisa/Desktop/my_grad_streamlit/myenvDone/lib/python3.12/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.2' (you have '2.0.1'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "from insightface.app import FaceAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/invisa/Desktop/my_grad_streamlit/insightface_model/models/buffalo_sc/det_500m.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/invisa/Desktop/my_grad_streamlit/insightface_model/models/buffalo_sc/w600k_mbf.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/invisa/Desktop/my_grad_streamlit/myenvDone/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:115: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "MODEL_ROOT = '/home/invisa/Desktop/my_grad_streamlit/insightface_model'\n",
    "MODEL_NAME = 'buffalo_sc'\n",
    "DETECTION_SIZE = (640, 640)\n",
    "RTSP_URL = \"rtsp://admin:Admin%40123@192.168.1.64:554/Streaming/Channels/102\"\n",
    "CHROMA_STORE_PATH = \"./store\"\n",
    "DATABASE_PATH = 'attendance_system.db'\n",
    "\n",
    "\n",
    "app = FaceAnalysis(name=MODEL_NAME, root=MODEL_ROOT, providers=['CUDAExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=DETECTION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime version:  1.20.1\n",
      "Available providers:  ['AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "print(\"ONNX Runtime version: \", ort.__version__)\n",
    "print(\"Available providers: \", ort.get_available_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unable to load from type '<class 'insightface.app.face_analysis.FaceAnalysis'>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(ort\u001b[38;5;241m.\u001b[39mget_available_providers())  \u001b[38;5;66;03m# Output: ['CUDAExecutionProvider', ...]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Use CUDA as the primary provider\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproviders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCUDAExecutionProvider\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAzureExecutionProvider\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCPUExecutionProvider\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/my_grad_streamlit/myenvDone/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:451\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_bytes \u001b[38;5;241m=\u001b[39m path_or_bytes  \u001b[38;5;66;03m# TODO: This is bad as we're holding the memory indefinitely\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load from type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(path_or_bytes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;241m=\u001b[39m sess_options\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options_initial \u001b[38;5;241m=\u001b[39m sess_options\n",
      "\u001b[0;31mTypeError\u001b[0m: Unable to load from type '<class 'insightface.app.face_analysis.FaceAnalysis'>'"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Check available providers (should now include CUDA)\n",
    "print(ort.get_available_providers())  # Output: ['CUDAExecutionProvider', ...]\n",
    "\n",
    "# Use CUDA as the primary provider\n",
    "session = ort.InferenceSession(\n",
    "    app,\n",
    "    providers=[\"CUDAExecutionProvider\", \"AzureExecutionProvider\", \"CPUExecutionProvider\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "print(ort.get_available_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Check available providers\n",
    "print(ort.get_available_providers())\n",
    "# Expected output: ['CUDAExecutionProvider', 'CPUExecutionProvider', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall onnxruntime-gpu -y\n",
    "pip install onnxruntime-gpu==1.18.0  # Explicitly supports CUDA 12.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/invisa/Desktop/my_grad_streamlit/insightface_model/models/buffalo_sc/det_500m.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/invisa/Desktop/my_grad_streamlit/insightface_model/models/buffalo_sc/w600k_mbf.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st# Constants\n",
    "from insightface.app import FaceAnalysis\n",
    "MODEL_ROOT = '/home/invisa/Desktop/my_grad_streamlit/insightface_model'\n",
    "MODEL_NAME = 'buffalo_sc'\n",
    "DETECTION_SIZE = (640, 640)\n",
    "RTSP_URL = \"rtsp://admin:Admin%40123@192.168.1.64:554/Streaming/Channels/102\"\n",
    "CHROMA_STORE_PATH = \"./store\"\n",
    "DATABASE_PATH = 'attendance_system.db'\n",
    "\n",
    "# Initialize face analysis model\n",
    "# try:\n",
    "app = FaceAnalysis(name=MODEL_NAME, root=MODEL_ROOT, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=DETECTION_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvDone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
